# -*- coding: utf-8 -*-
"""DatasetV3(CSV).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xMrOLsb5upl5Rikxd69Ianei-5eg3kwe
"""

# Commented out IPython magic to ensure Python compatibility.
#API key for gmail account: AIzaSyBb6pD6q8Jm3hu_DY_h1KKUip0ixrIyX5Q
#API key for bu account: AIzaSyA-fz-c4G86v6uuTR4JaEohG8uKjrUBP9c

APIKey = #add your own API key

from google.colab import drive
drive.mount('/content/drive')

!pip install google-api-python-client

# %cd '/content/drive/MyDrive/YTScoringDataSet/CSV Databases/'
!ls
folderPath = '/content/drive/MyDrive/YTScoringDataSet/CSV Databases/'


#Remember to change the add fucntion to all so add current date

APIKey = #add your own API key

#this function counts the number of examples in a table
import sqlite3
def count_examples(database_path, table_name):
    # Connect to the SQLite database
    conn = sqlite3.connect(database_path)
    cursor = conn.cursor()

    try:
        # Execute the SQL query to count the number of rows in the table
        cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
        count = cursor.fetchone()[0]
    except sqlite3.Error as e:
        print(f"An error occurred while counting examples: {e}")
        count = None
    finally:
        # Close the database connection
        cursor.close()
        conn.close()

    return count

def count_unique_examples(database_paths, table_name):
    unique_rows = set()
    
    # Connect to the SQLite databases and fetch rows
    for database_path in database_paths:
        conn = sqlite3.connect(database_path)
        cursor = conn.cursor()
        
        try:
            # Fetch all rows from the database's table
            cursor.execute(f"SELECT * FROM {table_name}")
            rows = cursor.fetchall()

            # Combine the sets of rows and remove duplicates
            unique_rows.update(rows)
        except sqlite3.Error as e:
            print(f"An error occurred while counting unique examples in '{database_path}': {e}")
        finally:
            # Close the database connections
            cursor.close()
            conn.close()

    # Count the unique rows
    unique_count = len(unique_rows)

    return unique_count

# Function to write video_data to a CSV file
def write_to_csv(video_data, file_name):
    with open(file_name, mode='w', newline='', encoding='utf-8') as csvfile:
        fieldnames = ['video_id', 'video_title', 'view_count', 'thumbnail_url', 'publish_date',]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

        writer.writeheader()
        for video in video_data:
            writer.writerow(video)

# Save video_data to a CSV file
#csv_file_name = 'youtube_video_data.csv'
#write_to_csv(video_data, csv_file_name)

# Function to append new video_data to a CSV file
def append_new_videos_to_csv(video_data, file_name, query, counter):
    existing_video_data = []

    # Read the existing CSV file and store the video data in a list
    try:
        with open(file_name, mode='r', newline='', encoding='utf-8') as csvfile:
            reader = csv.DictReader(csvfile)
            for row in reader:
                existing_video_data.append(row)
    except FileNotFoundError:
        pass  # If the file does not exist, we will create it later

    # Find the video IDs in the existing video data
    existing_video_ids = set(video['video_id'] for video in existing_video_data)

    # Filter out videos whose IDs are already in the existing video data
    new_video_data = [video for video in video_data if video['video_id'] not in existing_video_ids]

    print(f"Run #{counter}")

    # Count and print the number of new examples added
    num_new_examples = len(new_video_data)
    print(f"Number of new examples added for search {query}: {num_new_examples}")

    # Append new video data to the existing video data
    all_video_data = existing_video_data + new_video_data
    total_examples = len(all_video_data)
    print(f"Total number of examples: {total_examples}")

    # Write the updated video data to the CSV file
    with open(file_name, mode='w', newline='', encoding='utf-8') as csvfile:
        fieldnames = ['video_id', 'video_title', 'view_count', 'thumbnail_url', 'publish_date', 'date_data_acquired', 'days_published']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

        writer.writeheader()
        for video in all_video_data:
            writer.writerow(video)

'''search_queries = [# Technology and Programming'Python programming tutorial','Web development','Machine learning basics','Data science projects',
    # Cooking and Recipes'Vegan recipes',
    'Paleo recipes','Quick dinner ideas','Healthy breakfast options','Italian cuisine','Mexican cuisine',
    # Travel and Tourism'Top travel destinations','Budget travel tips','Cultural experiences','Road trip ideas',
    # Fitness and Health'Home workout routines','Yoga for beginners','Healthy lifestyle tips','Mental health and wellness',
    # Entertainment'Movie trailers','Music videos','Stand-up comedy','Video game reviews',# Science and Education'Physics experiments','DIY robotics projects','Astronomy for beginners','Environmental conservation',
    # Art and Design'Digital illustration techniques','Ceramics tutorial','Photography tips','Calligraphy and hand lettering',# Lifestyle and Hobbies'Gardening for beginners','Sustainable living ideas',
    # History and Culture'Ancient civilizations','Famous art movements','Cultural festivals around the world',
    # Personal Finance and Business'Investing for beginners','Entrepreneurship tips','Real estate investing','Budgeting and saving money',
    # Health and Medicine'Medical breakthroughs','Mental health therapies','Exercise physiology',
    # Language Learning,'Spanish language lessons','Sign language tutorial','Learning Chinese characters','English grammar exercises',
    # DIY and Home Improvement'Home renovation projects','Upcycling and repurposing','Interior design ideas',
    # Pets and Animals'Dog training tips','Aquarium setup and maintenance','Wildlife conservation efforts',
    # News and Politics
    'Global news highlights',
    'Political debates',
    'Climate change policies',
    'International relations analysis',

    # Spirituality and Philosophy
    'Meditation techniques',
    'World religions overview',
    'Existential philosophy',
    'Mindfulness practices',

    # Parenting and Family
    'Parenting tips and advice',
    'Educational activities for kids',

    # Sports and Athletics
    'Sports highlights',
    'Beginner workout routines',
    'Outdoor adventure sports',

    # Fashion and Beauty
    'Makeup tutorials',
    'Fashion trends and styling tips',
    'Haircare and styling',
    'Skincare routines and advice',

    # Careers and Education
    'Career advice and job search',
    'Study tips and strategies',
    'Resume and cover letter writing',

    'Piano lessons for beginners',
    'Guitar playing techniques',
    'Electronic music production',
    'World music and instruments',

    # Social Issues and Activism
    'Mental health awareness',
    'Community building and engagement',

    # Nature and Outdoors
    'Wildlife documentaries',
    'National parks and nature reserves',
    'Hiking and camping tips',
    'Nature photography techniques',

    # Food and Beverages
    'Wine tasting and appreciation',
    'Coffee brewing methods',
    'International cuisine recipes',
    'Dessert and baking ideas',

    # Space and Astronomy
    'Space exploration missions',
    'Astrophysics discoveries',
    'Rocket science and engineering',

    # Agriculture and Gardening
    'Urban farming techniques',
    'Sustainable agriculture practices',

    # Food and Beverages
    'Wine tasting and appreciation',
    'Coffee brewing methods',
    'International cuisine recipes',
    'Dessert and baking ideas',

    # Space and Astronomy
    'Space exploration missions',
    'Astrophysics discoveries',
    'Rocket science and engineering',

    # Agriculture and Gardening
    'Urban farming techniques',
    'Sustainable agriculture practices',
    'Indoor gardening and houseplants',

    # DIY and Crafts
    'Sewing and embroidery tutorials',
    'Paper crafts and origami',

    # Vehicles and Transportation
    'Car reviews and comparisons',
    'Public transportation systems',
    'Bicycle maintenance and repair',
    'Electric vehicles and green technology',

    # Dance and Performing Arts
    'Ballet techniques',
    'Hip-hop dance tutorials',
    'Theater and acting lessons',
    'Circus arts and acrobatics',
    
    'Knitting and crochet projects',
    'Jewelry making and beading',

    # Science and Research\
    'Biology and genetics',
    'Chemistry experiments',
    'Psychology and human behavior',

    # Literature and Writing
    'Creative writing tips',
    'Book reviews and recommendations',
    'Writing and publishing advice',

    # Travel and Adventure
    'Solo travel experiences',
    'Off-the-beaten-path destinations',
    'Cultural immersion and travel',
    'Eco-friendly travel tips',

    # Film and Media
    'Filmmaking techniques',
    'Documentary storytelling',
    'Cinematography and lighting',
    'Video editing tips',

    # Health and Fitness
    'How to run a marathon',
    'Sports nutrition advice',
    'Physical therapy and rehabilitation',

    # Hobbies and Games
    'Board game reviews',
    'Painting and drawing techniques',
    'Puzzle solving and strategy',

    # Technology and Gadgets
    'Smartphone reviews',
    'Home automation and smart devices',
    'Drone technology and usage',
    'Virtual reality experiences',

    # Personal Growth and Development
    'Public speaking tips',
    'Time management strategies',
    'Goal setting and motivation',
    'Emotional intelligence and communication',

    # Environment and Sustainability
    'Renewable energy innovations',
    'Zero waste lifestyle',

    # Architecture and Design
    'Modern architecture styles',
    'Landscape design ideas',
    'Sustainable building materials',
    'Urban planning and development',

    # History and Archaeology
    'Ancient artifacts discoveries',
    'Historical landmarks and sites',
    'Archaeological excavations',
    'Maritime history and shipwrecks',
    'Tesla',

    # Education and Teaching
    'Teaching strategies and methods',
    'Classroom management techniques',
    'Educational technology tools',
    'Student engagement and motivation',

    # Alternative Medicine and Wellness
    'Herbal remedies and supplements',
    'Acupuncture and traditional medicine',
    'Aromatherapy and essential oils',
    'Chiropractic and alternative therapies',
    'SpaceX',
    'Neuralink'

    search_queries_set13 = [
    'Underwater exploration and marine life',
    '3D printing projects and techniques',
    'Tiny homes and minimalist living',
    'Cryptography and code-breaking',
    'Culinary techniques and cooking tips',
    'Restorative yoga and relaxation',
    'Social media marketing strategies',
    'Language learning apps and resources',
    'Ethical fashion and fair trade',
    'Local and seasonal cooking',
]

]'''

import os
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError  # Import HttpError
import csv
import requests
from PIL import Image
from io import BytesIO
import datetime

api_key = APIKey
youtube = build('youtube', 'v3', developerKey=api_key)


search_queries = [
    'Neurallink'
    'Underwater exploration and marine life',
    '3D printing projects and techniques',
    'Tiny homes and minimalist living',
    'Cryptography and code-breaking',
    'Culinary techniques and cooking tips',
    'Restorative yoga and relaxation',
    'Social media marketing strategies',
    'Language learning apps and resources',
    'Ethical fashion and fair trade',
]

def AddToDataset(search_queries, counter):
    for search_query in search_queries:
      num_images = 500
      max_results_per_page = 50
      counter += 1
      if counter >= 10:
          print("Results and images were decreased")
          max_results_per_page = 45
          num_images = 400

      next_page_token = None
      video_data = []

      while len(video_data) < num_images:
          search_response = youtube.search().list(
              q=search_query,
              part='id',
              type='video',
              maxResults=max_results_per_page,
              videoDefinition='high',
              pageToken=next_page_token
          ).execute()

          next_page_token = search_response.get('nextPageToken')
          video_ids = [item['id']['videoId'] for item in search_response['items']]

          video_response = youtube.videos().list(
              id=','.join(video_ids),
              part='snippet,statistics,contentDetails',
              maxResults=max_results_per_page
          ).execute()

          for item in video_response['items']:
              video_id = item.get('id')
              video_title = item['snippet'].get('title', 'N/A')
              view_count = int(item['statistics'].get('viewCount', '-1'))
              thumbnail_url = item['snippet']['thumbnails']['high'].get('url', 'N/A')
              publish_date = item['snippet'].get('publishedAt', 'N/A')

              # You can also add a conditional check to exclude videos with missing data
              if video_id and video_title != 'N/A' and view_count != -1 and thumbnail_url != 'N/A' and publish_date != 'N/A':
                  # Parse publish_date to a datetime object
                  publish_date_dt = datetime.datetime.strptime(publish_date, '%Y-%m-%dT%H:%M:%SZ')
                  
                  # Get the current date and time
                  date_data_acquired = datetime.datetime.now()
                  
                  # Calculate the days_published
                  days_published = (date_data_acquired - publish_date_dt).total_seconds() / (60 * 60 * 24)

                  video_data.append({
                      'video_id': video_id,
                      'video_title': video_title,
                      'view_count': view_count,
                      'thumbnail_url': thumbnail_url,
                      'publish_date': publish_date,
                      'date_data_acquired': date_data_acquired.strftime('%Y-%m-%d %H:%M:%S'),
                      'days_published': days_published
                  })


              # Break the loop if we have reached the desired number of images
              if len(video_data) >= num_images:
                  break

      #for video in video_data:
          #print(video)

      # Example usage:
      csv_file_name = 'youtube_video_data.csv'
      append_new_videos_to_csv(video_data, csv_file_name, search_query, counter)

AddToDataset(search_queries, 0)

import pandas as pd
import matplotlib.pyplot as plt

def plot_views_per_day(csv_file):
    # Read the CSV file
    df = pd.read_csv(csv_file)

    # Calculate views_per_day
    df['views_per_day'] = df['view_count'] / df['days_published']

    # Plot the bar chart
    plt.figure(figsize=(10, 5))
    plt.bar(df.index, df['views_per_day'], color='b')
    plt.xlabel('Video Index')
    plt.ylabel('Views per Day')
    plt.title('Views per Day for Each Video')

    # Display the chart
    plt.show()


plot_views_per_day('youtube_video_data.csv')